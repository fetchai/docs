import {Callout} from 'nextra/components'
import PackageVersion from 'components/package-version'
import { CodeGroup, CodeSegment, DocsCode, GithubCodeSegment } from "../../../../components/code";

# Build a RAG Agent with uAgents and Langchain

## Introduction

In this guide, we'll walk through how to create [Agents ↗️](/guides/agents/getting-started/whats-an-agent) capable of answering questions based on any provided document using the Fetch.ai **uAgents** and **AI Engine** Python libraries as well as **OpenAI** and **Cohere**. The aim is to assist you in building a **LangChain Retrieval-Augmented Generation (RAG) Agent**!

    <Callout type="info" emoji="ℹ️">
      Also, rememeber that you can have a look and eventually download the source code used within this guide from Github [here ↗️](https://github.com/fetchai/uAgent-Examples/tree/main/1-uagents/examples/intermediate/langchain-rag/src).
    </Callout>  

The uAgents and AI Engine libraries offer a decentralized and modular framework for creating RAG Agents, improving the process of creating them. These tools streamline the integration of AI models like OpenAI and Cohere, enabling more efficient and scalable development of intelligent Agents. With enhanced interoperability, security, and resource management, this framework allows developers to quickly build and deploy sophisticated Agents that can effectively answer questions based on any provided document, making the entire process faster and more robust.

    <Callout type="info" emoji="ℹ️"> 
      Check out the [AI Engine package ↗️](https://pypi.org/project/uagents-ai-engine/) and [uAgents ↗️](https://pypi.org/project/uagents/) packages to download them and start integrating this tools within your Agents project!

      Current version of the AI Engine package is <PackageVersion packageName="uagents-ai-engine" packageType="pypi" />.
      Current version of the uAgents package is <PackageVersion packageName="uagents" packageType="pypi" />.
    </Callout>

**Let's dive into the LangChain RAG Agents development!**

## Prerequisites

Make sure you have read the following resources before going on with this guide:

    - [Creating an Agent ↗️](/guides/agents/create-a-uagent)
    - [Creating an interval task ↗️](/guides/agents/interval-task)
    - [Communicating with other Agents ↗️](/guides/agents/communicating-with-other-agents)
    - [Agent handlers ↗️](/guides/agents/intermediate/handlers)
    - [Almanac Contract ↗️](/references/contracts/uagents-almanac/almanac-overview)
    - [Register in Almanac ↗️](/guides/agents/register-in-almanac)
    - [Mailbox ↗️](/guides/agents/intermediate/mailbox)
    - [Utilising the Agentverse Mailroom service ↗️](/guides/agentverse/utilising-the-mailbox)
    - [Agents protocols ↗️](/guides/agents/intermediate/protocols)
    - [Agentverse Functions ↗️](/guides/agents/intermediate/agent-functions)
    - [Register an Agent Function on the Agentverse ↗️](/guides/agentverse/agentverse-functions/registering-agent-services)

### LangChain and Cohere Overview

#### LangChain

In the context of a RAG agent, LangChain helps by:

- **Document Loading and Parsing**: it processes and structures data from web pages.
- **Vector Storage**: LangChain indexes documents into vectors, making it easier to perform similarity searches.
- **Question Answering**: it generates answers by querying the indexed data, providing context-aware responses.

#### Cohere

Cohere offers **advanced NLP models** that assist in refining the answers generated by LangChain:

- **Contextual Compression**: it filters and prioritizes data based on relevance, ensuring that the Agent focuses on the most important information.
- **Enhanced Accuracy**: Cohere's NLP models improve the quality of the generated answers by understanding the context of the query more effectively.

#### How These Work Together in a RAG Agent

By combining **LangChain** for document retrieval and **Cohere** for NLP enhancement, this setup allows our Agents to process complex queries with context-aware responses, improving accuracy and user experience. LangChain handles the heavy lifting of document parsing and information retrieval, while Cohere fine-tunes the response to ensure high-quality, relevant answers.

## API KEYs

You will need **two API keys** to correctly go through this guide: 

    1. One from **OpenAI**
    2. One from **Cohere**. 

Follow the steps provided below to obtain them:

### OpenAI API Key

1. Visit the [OpenAI website ↗️](https://openai.com/).
2. Sign up or log in to your account.
3. Navigate to the **API** section.
4. Generate or retrieve your API key.

### Cohere API Key

1. Visit the [Cohere website ↗️](https://dashboard.cohere.com/).
2. Sign up or log in to your account.
3. Go to the **API Keys** section.
4. Copy an existing key or create a new one.

## Project structure and overview

### Project structure

The project is structured as follows:

    ```
    langchain-rag/
    .
    ├── poetry.lock
    ├── pyproject.toml
    └── src
        ├── agents
        │   ├── langchain_rag_agent.py
        │   └── langchain_rag_user.py
        ├── main.py
        └── messages
            └── requests.py
    ```

    The source code directory (`src`) contains the following directories and files:

        - `agents`: contains the scripts for the LangChain Agents.
            - `langchain_rag_agent.py`: it is the script for the RAG agent (retrieves info, finds documents, generates answers).
            - `langchain_rag_user.py`: Script for the User agent (asks questions, handles responses).
        - `main.py`: Starts both the RAG and user agents.
        - `messages`: Defines custom message models.
            - `requests.py`: Defines the RagRequest message model (question, URL, optional deep read).

### Environment variables

You'll need to set up environment variables for your project to run correctly. These variables include your API keys, which should be stored in a `.env` file within the `src` directory.

To do this, navigate to the `src` directory. Here, create and source the `.env` file. Within this one, add the following:

    ```
    export COHERE_API_KEY="YOUR_COHERE_API_KEY"
    export OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
    ```

## Langchain RAG setup

### Project dependencies

You'll need several Python packages for the project. These can be managed efficiently using [Poetry ↗️](https://python-poetry.org/). The following dependencies are required for the correct development of the Langchain RAG Agent project:

    ```
    [tool.poetry.dependencies]
    python = ">=3.10,<3.12"
    uagents = "^0.17.1"
    requests = "^2.31.0"
    langchain = "^0.3.7"
    openai = "^1.54.5"
    langchain-openai = "^0.2.9"
    tiktoken = "^0.8.0"
    cohere = "^5.11.4"
    faiss-cpu = "^1.9.0.post1"
    validators = "^0.34.0"
    uagents-ai-engine = "^0.6.0"
    unstructured = "^0.16.5"
    langchain-community = "^0.3.7"
    ```

## Messages Data Model

### RagRequest Model

We now need to define the `requests.py` file under the `messages` folder in the project:

    <CodeGroup hasCopy isOSFile>
    <DocsCode mac={true}>
        ```py copy filename="mac"
        touch requests.py
        ```
    </DocsCode>

    <DocsCode windows={true}>
        ```py copy filename="windows"
        echo. > requests.py
        ```
    </DocsCode>

    <DocsCode ubuntu={true}>
        ```py copy filename="ubuntu"
        touch requests.py
        ```
    </DocsCode>
    </CodeGroup>

The script looks like the following:

<GithubCodeSegment digest="6d99301cf7b5b94c82d8cc447a1891fe">
    <CodeSegment
        path="https://github.com/fetchai/uAgent-Examples/blob/main/1-uagents/examples/intermediate/langchain-rag/src/messages/requests.py"
        lineStart={1}
        lineEnd={14}
        hosted={true}
    />
</GithubCodeSegment>
<CodeGroup dynamic hasCopy isLocalHostedFile digest='6d99301cf7b5b94c82d8cc447a1891fe'>

<DocsCode local={true}>
	```py copy filename="requests.py"

	from typing import Optional
	from uagents import Model, Field
	 
	 
	class RagRequest(Model):
	    question: str = Field(
	        description="The question that the user wants to have an answer for."
	    )
	    url: str = Field(description="The url of the docs where the answer is.")
	    deep_read: Optional[str] = Field(
	        description="Specifies weather all nested pages referenced from the starting URL should be read or not. The value should be yes or no.",
	        default="no",
	    )
	 

```
</DocsCode>

</CodeGroup>



    Here's a breakdown of the `RagRequest` message data model:

        - `question (str)`: it is the user's question that needs to be answered based on the provided website URL.
        - `url (str)`: it is the URL of the website where the answer should be found.
        - `deep_read (Optional[str], default="no")`: this optional field allows you to specify whether the RAG Agent should follow and read nested pages (i.e., pages linked from the starting URL). Valid values are `"yes"` or `"no"`. By `default`, it is set to `"no"`, meaning that the Agent only will focus on the initial page linked to the URL.

## Agents

### LangChain RAG Agent

This step involves setting up the **LangChain Retrieval-Augmented Generation (RAG) Agent** which can scrape web content, retrieve relevant documents, and generate answers to user queries based on the retrieved content. 

Let's start by creating the file within the `agents` directory, under `src` directory of our project, and name it `langchain_rag_agent.py` by using the following command:

    <CodeGroup hasCopy isOSFile>
    <DocsCode mac={true}>
        ```py copy filename="mac"
        touch langchain_rag_agent.py
        ```
    </DocsCode>

    <DocsCode windows={true}>
        ```py copy filename="windows"
        echo. > langchain_rag_agent.py
        ```
    </DocsCode>

    <DocsCode ubuntu={true}>
        ```py copy filename="ubuntu"
        touch langchain_rag_agent.py
        ```
    </DocsCode>
    </CodeGroup>

The Agent is defined as a [local agent with a Mailbox ↗️](https://fetch.ai/docs/guides/agents/intermediate/options-for-running-local-agents#run-a-local-agent-using-a-mailbox) and it can answer questions by fetching and summarizing information from a given website URL.

The script for this Agent looks as follows:

<GithubCodeSegment digest="6e1bac7fcd2e972c9e57b0baae58c27c">
    <CodeSegment
        path="https://github.com/fetchai/uAgent-Examples/blob/main/1-uagents/examples/intermediate/langchain-rag/src/agents/langchain_rag_agent.py"
        lineStart={1}
        lineEnd={137}
        hosted={true}
    />
</GithubCodeSegment>
<CodeGroup dynamic hasCopy isLocalHostedFile digest='6e1bac7fcd2e972c9e57b0baae58c27c'>

<DocsCode local={true}>
	```py copy filename="langchain_rag_agent.py"

	import traceback
	from uagents import Agent, Context, Protocol
	import validators
	from messages.requests import RagRequest
	import os
	from langchain_openai import ChatOpenAI
	from langchain.prompts import ChatPromptTemplate
	from langchain_community.document_loaders import UnstructuredURLLoader
	import requests
	from bs4 import BeautifulSoup
	from urllib.parse import urlparse
	from langchain_openai import OpenAIEmbeddings
	from langchain_community.vectorstores import FAISS
	from langchain.retrievers import ContextualCompressionRetriever
	from langchain.retrievers.document_compressors import CohereRerank
	from ai_engine import UAgentResponse, UAgentResponseType
	import nltk
	from uagents.setup import fund_agent_if_low
	 
	nltk.download("punkt")
	nltk.download("averaged_perceptron_tagger")
	 
	 
	LANGCHAIN_RAG_SEED = "YOUR_LANGCHAIN_RAG_SEED"
	AGENT_MAILBOX_KEY = "YOUR_MAILBOX_KEY"
	 
	agent = Agent(
	    name="langchain_rag_agent",
	    seed=LANGCHAIN_RAG_SEED,
	    mailbox=f"{AGENT_MAILBOX_KEY}@https://agentverse.ai",
	)
	fund_agent_if_low(agent.wallet.address())
	 
	docs_bot_protocol = Protocol("DocsBot")
	 
	 
	PROMPT_TEMPLATE = """
	Answer the question based only on the following context:
	 
	{context}
	 
	---
	 
	Answer the question based on the above context: {question}
	"""
	 
	 
	def create_retriever(
	    ctx: Context, url: str, deep_read: bool
	) -> ContextualCompressionRetriever:
	    def scrape(site: str):
	        if not validators.url(site):
	            ctx.logger.info(f"Url {site} is not valid")
	            return
	 
	        r = requests.get(site)
	        soup = BeautifulSoup(r.text, "html.parser")
	 
	        parsed_url = urlparse(url)
	        base_domain = parsed_url.scheme + "://" + parsed_url.netloc
	 
	        link_array = soup.find_all("a")
	        for link in link_array:
	            href: str = link.get("href", "")
	            if len(href) == 0:
	                continue
	            current_site = f"{base_domain}{href}" if href.startswith("/") else href
	            if (
	                ".php" in current_site
	                or "#" in current_site
	                or not current_site.startswith(url)
	                or current_site in urls
	            ):
	                continue
	            urls.append(current_site)
	            scrape(current_site)
	 
	    urls = [url]
	    if deep_read:
	        scrape(url)
	        ctx.logger.info(f"After deep scraping - urls to parse: {urls}")
	 
	    try:
	        loader = UnstructuredURLLoader(urls=urls)
	        docs = loader.load_and_split()
	        db = FAISS.from_documents(docs, OpenAIEmbeddings())
	        compression_retriever = ContextualCompressionRetriever(
	            base_compressor=CohereRerank(), base_retriever=db.as_retriever()
	        )
	        return compression_retriever
	    except Exception as exc:
	        ctx.logger.error(f"Error happened: {exc}")
	        traceback.format_exception(exc)
	 
	 
	@docs_bot_protocol.on_message(model=RagRequest, replies={UAgentResponse})
	async def answer_question(ctx: Context, sender: str, msg: RagRequest):
	    ctx.logger.info(f"Received message from {sender}, session: {ctx.session}")
	    ctx.logger.info(
	        f"input url: {msg.url}, question: {msg.question}, is deep scraping: {msg.deep_read}"
	    )
	 
	    parsed_url = urlparse(msg.url)
	    if not parsed_url.scheme or not parsed_url.netloc:
	        ctx.logger.error("invalid input url")
	        await ctx.send(
	            sender,
	            UAgentResponse(
	                message="Input url is not valid",
	                type=UAgentResponseType.FINAL,
	            ),
	        )
	        return
	    base_domain = parsed_url.scheme + "://" + parsed_url.netloc
	    ctx.logger.info(f"Base domain: {base_domain}")
	 
	    retriever = create_retriever(ctx, url=msg.url, deep_read=msg.deep_read == "yes")
	 
	    compressed_docs = retriever.get_relevant_documents(msg.question)
	    context_text = "\n\n---\n\n".join([doc.page_content for doc in compressed_docs])
	    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
	    prompt = prompt_template.format(context=context_text, question=msg.question)
	 
	    model = ChatOpenAI(model="gpt-4o-mini")
	    response = model.predict(prompt)
	    ctx.logger.info(f"Response: {response}")
	    await ctx.send(
	        sender, UAgentResponse(message=response, type=UAgentResponseType.FINAL)
	    )
	 
	 
	agent.include(docs_bot_protocol, publish_manifest=True)
	 
	 
	if __name__ == "__main__":
	    agent.run()
	 

```
</DocsCode>

</CodeGroup>



    The Agent fetches and processes information from specified URLs to answer users' questions. 

    Let's explore the script above in more detail. We start by importing multiple modules, including tools for making HTTP requests, parsing HTML, and handling Natural Language Processing (NLP). The Agent uses **OpenAI's GPT-4o-mini model** to generate answers based on the retrieved documents. We then create an Agent called `langchain_rag_agent()` with a unique `seed` and `mailbox` address. Check out the [Mailbox ↗️](/guides/agents/intermediate/mailbox) guide for additional information on how to set up Mailboxes for your Agents. **Make sure to have enough funds in the Agent's wallet for it to correctly register in the [Almanac contract ↗️](/references/contracts/uagents-almanac/almanac-overview). You can use the `fund_agent_if_low` function on the testnet to add funds to your Agent's wallet if needed.

    Next, we define the `DocsBot` [protocol ↗️](/guides/agents/intermediate/protocols) using the `Protocol` class of the `uagents` library to handle message interactions. This protocol uses a predefined prompt template to structure the questions and context for the language model.

    The `create_retriever()` function is responsible for fetching and parsing web pages. If necessary, it can perform deep scraping, which involves gathering all linked pages within the same domain. The function validates URLs, retrieves HTML content, and uses `BeautifulSoup` to parse the HTML and find links. The documents are then loaded and split using LangChain's `UnstructuredURLLoader`, indexed with `FAISS`, and compressed with `Cohere`, creating a retriever that can extract relevant information.

    The `answer_question()` function, decorated with the `.on_message()` [handler ↗️](/guides/agents/intermediate/handlers), is triggered when the Agent receives a message matching the `RagRequest` message data model. The function validates the input URL and creates a retriever to fetch relevant documents based on the question. The context alongside with the question are then used to create a prompt for the language model (`ChatOpenAI`), which generates the final answer.

    The generated answer is then sent back to the user.

    Finally, the Agent is set to include the `DocsBot` protocol. We can then run the Agent's script; it will start the Agent and allow it to receive and process incoming messages.

    **To correctly run the code, you need to provide the `name`, `seed`, `mailbox`, `LANGCHAIN_RAG_SEED`, `PROMPT_TEMPLATE` and `AGENT_MAILBOX_KEY` parameters.**

### LangChain User Agent

We are now ready to define the **LangChain User Agent** which interacts with the LangChain RAG Agent we defined above to ask a question based on documents found at specified URLs. The user Agent request the RAG Agent to retrieve and process information from the web page and then to handle the response. The Agent is defined as a [local Agent with an endpoint ↗️](https://fetch.ai/docs/guides/agents/intermediate/options-for-running-local-agents#run-a-local-agent-with-an-endpoint).

Create a file for this Agent:

    <CodeGroup hasCopy isOSFile>
    <DocsCode mac={true}>
        ```py copy filename="mac"
        touch langchain_rag_user.py
        ```
    </DocsCode>

    <DocsCode windows={true}>
        ```py copy filename="windows"
        echo. > langchain_rag_user.py
        ```
    </DocsCode>

    <DocsCode ubuntu={true}>
        ```py copy filename="ubuntu"
        touch langchain_rag_user.py
        ```
    </DocsCode>
    </CodeGroup>

The script looks as follows:

<GithubCodeSegment digest="67ca92154bcb99f3c17512c6210727a4">
    <CodeSegment
        path="https://github.com/fetchai/uAgent-Examples/blob/main/1-uagents/examples/intermediate/langchain-rag/src/agents/langchain_rag_user.py"
        lineStart={1}
        lineEnd={43}
        hosted={true}
    />
</GithubCodeSegment>
<CodeGroup dynamic hasCopy isLocalHostedFile digest='67ca92154bcb99f3c17512c6210727a4'>

<DocsCode local={true}>
	```py copy filename="langchain_rag_user.py"

	from uagents import Agent, Context, Protocol
	from messages.requests import RagRequest
	from ai_engine import UAgentResponse
	from uagents.setup import fund_agent_if_low
	 
	 
	QUESTION = "How to install uagents using pip"
	URL = "https://fetch.ai/docs/guides/agents/installing-uagent"
	DEEP_READ = (
	    "no"
	)
	 
	RAG_AGENT_ADDRESS = "YOUR_LANGCHAIN_RAG_AGENT_ADDRESS"
	 
	user = Agent(
	    name="langchain_rag_user",
	    port=8000,
	    endpoint=["http://127.0.0.1:8000/submit"],
	)
	fund_agent_if_low(user.wallet.address())
	rag_user = Protocol("LangChain RAG user")
	 
	 
	@rag_user.on_interval(60, messages=RagRequest)
	async def ask_question(ctx: Context):
	    ctx.logger.info(
	        f"Asking RAG agent to answer {QUESTION} based on document located at {URL}, reading nested pages too: {DEEP_READ}"
	    )
	    await ctx.send(
	        RAG_AGENT_ADDRESS, RagRequest(question=QUESTION, url=URL, deep_read=DEEP_READ)
	    )
	 
	 
	@rag_user.on_message(model=UAgentResponse)
	async def handle_data(ctx: Context, sender: str, data: UAgentResponse):
	    ctx.logger.info(f"Got response from RAG agent: {data.message}")
	 
	 
	user.include(rag_user)
	 
	if __name__ == "__main__":
	    rag_user.run()
	 

```
</DocsCode>

</CodeGroup>



    Here, we create the User Agent which interacts with the LangChain RAG Agent we previously defined. The User Agent periodically asks the RAG Agent a predefined question and then handles the response.

    After importing all required modules and classes, a specific `QUESTION` is provided: `"How to install uagents using pip"`. We also provide the `URL` for the web page where the relevant information can be found to answer the question. The `DEEP_READ` variable is set to `no`; this indicates that the Agent should only read the main page.

    The `RAG_AGENT_ADDRESS` variable holds the address of the RAG Agent responsible of retrieving and processing the web page content to answer the user's question.

    We can then define the User Agent. We create the `langchain_rag_user` with a specific `port` and `endpoint`; this last one is essential to allow for communication with the RAG Agent. **Make sure your Agent has enough funds to register in the Almanac contract**. On the Testnet, you can use the `fund_agent_if_low()` function to fund your Agent's wallet if needed.

    We then proceed and define the [protocol ↗️](/references/uagents/uagents-protocols/agent-protocols). A protocol named `LangChain RAG user` is created using the `Protocol` class from the `uagents` library to handle the Agent's interactions. This protocol contains two important functions:

        1. The `ask_question()` function, which is decorated with the `.on_interval()` [handler ↗️](/guides/agents/intermediate/handlers) and which is set to run at 60-second intervals. This function sends a `RagRequest` message to the LangChain RAG Agent, asking it to answer the `QUESTION` based on the specified `URL`. It then logs the `QUESTION` and `URL` details for debugging purposes.

        2. The `handle_data()` function is decorated with the `.on_message()` handler and it handles incoming messages from the LangChain RAG Agent. When a response is received, it logs the response message.

    Finally, the LangChain RAG user protocol is included using the `.include()` method into the Agent which is then run by calling `rag_user.run()` in the main block.

    **Remember to provide the `QUESTION`, `URL`, `DEEP_READ`, `RAG_AGENT_ADDRESS`, `name`, `seed`  and `endpoint` parameters to correctly run this code.**

## Main script

We are now ready to define the main script for our project. In the `src` folder of the project, we create a Python script named `main.py` using the following command:

    <CodeGroup hasCopy isOSFile>
    <DocsCode mac={true}>
        ```py copy filename="mac"
        touch main.py
        ```
    </DocsCode>

    <DocsCode windows={true}>
        ```py copy filename="windows"
        echo. > main.py
        ```
    </DocsCode>

    <DocsCode ubuntu={true}>
        ```py copy filename="ubuntu"
        touch main.py
        ```
    </DocsCode>
    </CodeGroup>

We can now write the code for it. 

It will look as follows:

<GithubCodeSegment digest="9018aca08a2c6ffee127165c2cff6043">
    <CodeSegment
        path="https://github.com/fetchai/uAgent-Examples/blob/main/1-uagents/examples/intermediate/langchain-rag/src/main.py"
        lineStart={1}
        lineEnd={12}
        hosted={true}
    />
</GithubCodeSegment>
<CodeGroup dynamic hasCopy isLocalHostedFile digest='9018aca08a2c6ffee127165c2cff6043'>

<DocsCode local={true}>
	```py copy filename="main.py"

	from uagents import Bureau
	from agents.langchain_rag_agent import agent
	from agents.langchain_rag_user import user
	 
	 
	if __name__ == "__main__":
	    bureau = Bureau(endpoint="http://127.0.0.1:8000/submit", port=8000)
	    print(f"Adding RAG agent to Bureau: {agent.address}")
	    bureau.add(agent)
	    print(f"Adding user agent to Bureau: {user.address}")
	    bureau.add(user)
	    bureau.run()

```
</DocsCode>

</CodeGroup>



Cool! All scripts are now set up! You will need to [connect the local Agent to the Agentverse using a Mailbox ↗️](/guides/agents/intermediate/options-for-running-local-agents#run-a-local-agent-using-a-mailbox). The Mailbox enables your Agents to be retrievable for communication and interaction with any other registered Agent on the Almanac contract and Fetch Network. In this example, the `langchain_rag_agent` will be connected to the Agentverse using a [Mailbox ↗️](/guides/agents/intermediate/mailbox). The `langchain_rag_user` Agent is used as a testing Agent for the RAG Agent being registered on the Agentverse and made subsequently available on DeltaV for queries.

By creating an [Agent Function ↗️](https://fetch.ai/docs/guides/agents/intermediate/agent-functions) and [registering it ↗️](https://fetch.ai/docs/guides/agentverse/agentverse-functions/registering-agent-services) via the Agentverse, you will make it retrievable to the [AI Engine ↗️](/concepts/ai-engine/ai-engine-intro) for interaction with users via natural language queries made on [DeltaV ↗️](/concepts/ai-engine/deltav).

The `langchain_rag_user` Agent is used as a testing Agent for the RAG Agent being registered on the Agentverse and made subsequently available on DeltaV for queries.

### Expected output

The expected output for this example should be similar to the following one. Here, we questioned the **RAG Agent** with the question `How to install uagents using pip` from the **RAG User** Agent:

![](src/images/examples/rag_deltav_response.png)
